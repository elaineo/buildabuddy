{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "import nltk\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from semanticparser import *\n",
    "from tools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "username = 'hollymadison'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/%s.txt' % username) as docs_file:\n",
    "    documents = docs_file.read().replace('\\n', ' ').decode('utf-8')\n",
    "\n",
    "# break into sentences\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "sents_all = sent_detector.tokenize(documents.strip())\n",
    "sent_generator = nltk.bigrams(sents_all)\n",
    "sents = [\" \".join(s) for s in sent_generator]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = remove_stopwords(sents, True)\n",
    "dictionary, lsi, index = texts_to_index(texts, 256, username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/%s.json' % username, 'w') as f:\n",
    "    json.dump(sents,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary.save('data/%s.dict' % username) \n",
    "lsi.save('data/%s-corpus.lsi' % username)\n",
    "index.save('data/%s-corpus.index' % username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### test functions\n",
    "def load_sents(username, root='data'):\n",
    "    \"\"\" Load documents\n",
    "            Preprocessed: dictionary, corpus, index, lsi\n",
    "            Archives: documents\n",
    "    \"\"\"\n",
    "    dictionary = corpora.Dictionary.load('%s/%s.dict' % (root,username))\n",
    "\n",
    "    with open('%s/%s.json' % (root,username)) as docs_file:\n",
    "        documents = json.load(docs_file)\n",
    "\n",
    "    lsi = models.LsiModel.load('%s/%s-corpus.lsi' % (root,username))\n",
    "    #index = similarities.MatrixSimilarity.load('data/%s-corpus.index' % username)\n",
    "    index = similarities.Similarity.load('%s/%s-corpus.index' % (root,username))\n",
    "\n",
    "    return documents, dictionary, lsi, index\n",
    "\n",
    "def test_response(username, t):\n",
    "    trash = [t]\n",
    "    documents0, dictionary0, lsi0, index0 = load_sents(username)\n",
    "    r = gen_response(documents0, dictionary0, lsi0, index0, t, trash, True)\n",
    "    return r\n",
    "\n",
    "def gen_response(sents, dictionary, lsi, index, t, trash, limit1=True):\n",
    "    # tokenize input sentence\n",
    "    clean_input = clean_str(t).lower().split()\n",
    "\n",
    "    # get most similar post from input sentence\n",
    "    sims = query_page(clean_input, dictionary, lsi, index)\n",
    "\n",
    "    # repeat the process on the sentences in the doc\n",
    "    sample = [sents[sims[0][0]], sents[sims[1][0]], sents[sims[2][0]], sents[sims[3][0]],\n",
    "                sents[sims[4][0]], sents[sims[5][0]], sents[sims[6][0]], sents[sims[7][0]],\n",
    "                sents[sims[8][0]], sents[sims[9][0]]]\n",
    "\n",
    "    rmult = []\n",
    "    #reply = create_reply(sample, '')\n",
    "    if sample and sample not in trash:\n",
    "        # reply to the tweet\n",
    "        if limit1:\n",
    "            return sample\n",
    "        else:\n",
    "            rmult.append(sample)\n",
    "    if limit1:\n",
    "        return None\n",
    "    else:\n",
    "        return rmult\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'The dating scene in Las Vegas was pretty grim. During my three years as a single lady, I had the worst luck in the dating department.',\n",
       " u'Maybe some of the guys got a perverted little kick out of the fact that they were dating the same girl as Hugh Hefner. Who knows?',\n",
       " u'The fact that he was dating someone 22 years younger than him, not to mention barely legal, grossed me out since it reminded me so much of the Hefner situation. However, Criss seemed so sincere when he told me that he felt like he\\u2019d made a mistake and was looking for someone different that I was willing to overlook the impression his dating situation had made on me.',\n",
       " u'He told me he made a mistake getting together with her and was planning on breaking it off in the nicest way possible. The fact that he was dating someone 22 years younger than him, not to mention barely legal, grossed me out since it reminded me so much of the Hefner situation.',\n",
       " u'During my three years as a single lady, I had the worst luck in the dating department. I swear, I could write a book on the types of douchebags that lurk around these days (maybe I will!).',\n",
       " u'I have to admit, I felt a little slighted. Pictures of Tina, Brande, and Kimberley littered Hef\\u2019s room for years after I had moved in and Crystal was already throwing fits about any sign of me just a few months into dating Hef.',\n",
       " u'Most of them referred to \\u201cdating\\u201d Hef as a   big publicity stunt to help them launch their careers. Maybe some of the guys got a perverted little kick out of the fact that they were dating the same girl as Hugh Hefner.',\n",
       " u'We spoke about my relationship with Hef and how stifling it was. He confided in me that he started dating an 18-year-old girl who moved into his hotel suite and \\u201cwon\\u2019t move out,\\u201d as he put it.',\n",
       " u'He confided in me that he started dating an 18-year-old girl who moved into his hotel suite and \\u201cwon\\u2019t move out,\\u201d as he put it. He told me he made a mistake getting together with her and was planning on breaking it off in the nicest way possible.',\n",
       " u'It was during one such evening, after his separation, that Hef met Sandy and Mandy Bentley. Immediately he began dating these two blond bombshells, along with another blonde, Brande Roderick.']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_response(username, \"dating advice?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topics = [d[1] for d in dictionary.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'fawn',\n",
       " u'\\u201cpeeping\\u201d',\n",
       " u'yellow',\n",
       " u'four',\n",
       " u'askew',\n",
       " u'woods',\n",
       " u'hanging',\n",
       " u'marching',\n",
       " u'looking',\n",
       " u'granting']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate dialog files\n",
    "dump = '<?xml version=\"1.0\" encoding=\"UTF-8\"?><dialog xsi:noNamespaceSchemaLocation=\"WatsonDialogDocument_1.0.xsd\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"><flow><folder label=\"Main\">'\n",
    "dump = \"\"\n",
    "documents0, dictionary0, lsi0, index0 = load_sents(username)\n",
    "for top in topics[:1000]:\n",
    "    dump += '<input><grammar><item>*%s*</item></grammar><output><prompt selectionType=\"RANDOM\">' % top\n",
    "    responses = gen_response(sents, dictionary, lsi, index, top, [], True)\n",
    "    for r in responses:\n",
    "        dump += \"<item>%s</item>\" % r\n",
    "    dump += \"</prompt></output></input>\"\n",
    "dump += '</folder></flow></dialog>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('data/%s.xml' % username, 'w') as f:\n",
    "    json.dump(dump,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
